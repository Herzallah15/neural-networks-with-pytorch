{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115725270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a0ead-bc14-4d64-99a4-d89ae7c7a1ef",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Neural network layers are the fundamental building blocks that transform input data into meaningful representations. Each layer type is designed for specific data structures and tasks. The **shape of the input and output tensors** plays a crucial role in understanding how these layers operate.\n",
    "\n",
    "In what follows, we investigate the most common layer types in PyTorch:\n",
    "\n",
    "1. **nn.Linear**: Fully connected layers for general-purpose transformations\n",
    "2. **nn.Conv1d**: 1D convolutions for sequential/temporal data\n",
    "3. **nn.Conv2d**: 2D convolutions for image data\n",
    "4. **nn.Conv3d**: 3D convolutions for volumetric/video data\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19674a-3b68-45ec-923f-911e7192c272",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.Linear** | $(*, H_{\\text{in}})$ | $(*, H_{\\text{out}})$ | MLPs, classification heads, dense connections, Transformer projections | `in_features`, `out_features`, `bias` |\n",
    "| **nn.Conv1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Time series, audio, text (1D sequences) | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` |\n",
    "| **nn.Conv2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Images, feature maps, 2D spatial data | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "| **nn.Conv3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Videos, medical imaging (CT/MRI), 3D point clouds | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "\n",
    "Detailed explanations of each layer, including mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913c152-b2d7-401b-b573-3dc034a3956d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184a791-9132-4eb1-9ea3-1b2dbb235a57",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "### Where is it used?\n",
    "\n",
    "The `nn.Linear` layer (also known as a fully connected or dense layer) applies a linear transformation to the incoming data. It is the most fundamental building block in neural networks and is used in:\n",
    "\n",
    "- **Multi-Layer Perceptrons (MLPs)**: The backbone of simple feedforward networks\n",
    "- **Classification heads**: Final layers that map features to class logits\n",
    "- **Transformer architectures**: Q, K, V projections and feed-forward networks\n",
    "- **Autoencoders**: Encoding and decoding dense representations\n",
    "- **Regression tasks**: Mapping features to continuous outputs\n",
    "\n",
    "### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(*, H_{\\text{in}})$ where $*$ means any number of dimensions and $H_{\\text{in}}$ is the number of input features\n",
    "- **Output**: $(*, H_{\\text{out}})$ where all dimensions except the last remain unchanged\n",
    "\n",
    "**Important**: The linear transformation is applied to the **last dimension** only.\n",
    "\n",
    "### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "- `in_features` (int): Size of each input sample (required)\n",
    "- `out_features` (int): Size of each output sample (required)\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- \n",
    "### Mathematical Formulation\n",
    "\n",
    "The linear layer applies the following transformation:\n",
    "\n",
    "$$\n",
    "{y_{\\rm out}}^{i,j,\\cdots, k} = \\omega_{k z} \\, {x_{\\rm in}}^{i,j,\\cdots, z} + b^{k}\n",
    "$$\n",
    "\n",
    "\n",
    "where the indices \"$i,j,\\cdots$\" represent an arbitrary dimension, i.e., $*$. This makes the layer **batch-agnostic**. As we see,\n",
    "- $x$ is the input tensor of shape $(*, H_{\\text{in}})$\n",
    "- $\\omega$ is the weight matrix of shape $(H_{\\text{out}}, H_{\\text{in}})$\n",
    "- $b$ is the bias vector of shape $(H_{\\text{out}})$\n",
    "- $y$ is the output tensor of shape $(*, H_{\\text{out}})$\n",
    "- **Notice**, that, all elements in the arbirary dimension \"$i,j,\\cdots$\" get exactly the same biases and weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Weights are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{H_{\\text{in}}}$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62e6aaa-46a3-4f58-b540-ccab2157f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([30, 20])\n",
      "Bias shape: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Linear usage\n",
    "# Create a linear layer that maps 20 input features to 30 output features\n",
    "linear = nn.Linear(20, 30)\n",
    "\n",
    "# Check the weight and bias shapes\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d299c4-0284-4d7a-b5e5-01cb687c227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 20])\n",
      "Output shape: torch.Size([128, 30])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple 2D input (batch_size, in_features)\n",
    "batch_size = 128\n",
    "in_features = 20\n",
    "out_features = 30\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "models_weights = linear.weight\n",
    "models_biases = linear.bias\n",
    "x = torch.randn(batch_size, in_features)\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Lets now try to reproduce the output manually:\n",
    "\n",
    "output_manual = torch.einsum('bi,oi->bo', x, models_weights) + models_biases.unsqueeze(0).expand(batch_size, -1)\n",
    "print(f'Outputs match: {torch.all(output_manual==output).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b58cd32-e699-4cd3-be86-d44e5e910935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has bias: False\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Without bias\n",
    "linear_no_bias = nn.Linear(20, 30, bias=False)\n",
    "print(f\"Has bias: {linear_no_bias.bias is not None}\")\n",
    "\n",
    "# Verify the transformation manually\n",
    "x = torch.randn(5, 20)\n",
    "output_layer = linear_no_bias(x)\n",
    "output_manual = x @ linear_no_bias.weight.T  # y = x @ W^T\n",
    "\n",
    "print(f\"Outputs match: {torch.allclose(output_layer, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656ff2-0304-4bba-92b6-d911349b9487",
   "metadata": {},
   "source": [
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb845e-958c-4d18-ac16-ffdd8708b9b6",
   "metadata": {},
   "source": [
    "Before we start discussing the types of convolutional layers, we briefly discuss some of the important padding types appearing in these layers:\n",
    "\n",
    "\n",
    "\n",
    "| Padding | Conv1d | Conv2d | Conv3d | Meaning |\n",
    "|---------|--------|--------|--------|---------|\n",
    "| `padding=0` | `0` | `(0,0)` | `(0,0,0)` | No padding, output shrinks |\n",
    "| `padding='valid'` | `0` | `(0,0)` | `(0,0,0)` | Equivalent to `padding=0` |\n",
    "| `padding='same'` | auto | auto | auto | Output size = Input size (requires `stride=1`) |\n",
    "| `padding=k//2` | `k//2` | `(k//2, k//2)` | `(k//2, k//2, k//2)` | Preserves size for odd kernel $k$ with `stride=1`, `dilation=1` |\n",
    "\n",
    "**Common recipe to preserve spatial dimensions:**\n",
    "\n",
    "For `kernel_size=k`, `stride=1`, `dilation=1`:\n",
    "```python\n",
    "# These are equivalent:\n",
    "padding = 'same'\n",
    "padding = k // 2   # only works for odd k\n",
    "```\n",
    "\n",
    "**Examples preserving size:**\n",
    "\n",
    "| Kernel | Padding |\n",
    "|--------|---------|\n",
    "| 3 | 1 |\n",
    "| 5 | 2 |\n",
    "| 7 | 3 |\n",
    "\n",
    "**General formula:** `padding = (kernel_size - 1) // 2` for odd kernels with `stride=1`, `dilation=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd3a3e-f067-4665-9c67-47d3cea77666",
   "metadata": {},
   "source": [
    "### nn.Conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6b5e4-7460-4208-a79a-f9afa8a79915",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv1d` layer applies a 1D convolution over an input signal composed of several input planes. It is primarily used for:\n",
    "\n",
    "- **Time series analysis**: Stock prices, sensor data, weather patterns\n",
    "- **Audio processing**: Speech recognition, music classification\n",
    "- **Text/NLP**: Character-level or word-level sequence modeling\n",
    "- **Signal processing**: Any 1D sequential data\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, L_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $L_{\\text{in}}$ = length of the input sequence\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, L_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $L_{\\text{out}} = \\left\\lfloor\\frac{L_{\\text{in}} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int): Stride of the convolution. Default: `1`\n",
    "- `padding` (int or str): Zero-padding added to both sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size})$\n",
    "\n",
    "\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, L)$ and output of size $(B, C_{\\text{out}}, L_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, i]$ to $y_{\\rm out}[b, c, i] $ is performed as follows:\n",
    "\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s*i+~d*k] \\omega[c_{\\rm out}, c_{\\rm in}, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})*\\frac{C_{\\rm in}}{g} + j,~ s*i+~d*k] \\omega[c_{\\rm out}, j, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor, it is simply equal to $x_{\\rm in}$ if padding is zero. Moreover, $g = $ `groups`, $s = $ `stride`, $d = $ `dilation` , and $m(c_{\\rm out}) = {\\rm floor}( c_{out} * g / C_{out} )$.\n",
    "\n",
    "\n",
    "\n",
    "**Remark**: When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interactio.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b7daae-f2d9-4635-8b7b-0a0c813d89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv1d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3\n",
    "conv1d = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv1d.weight.shape}\")  # (out_channels, in_channels, kernel_size)\n",
    "print(f\"Bias shape: {conv1d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef7343c-7a6b-470d-b2f7-0aa8d0f6314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 48])\n",
      "Expected L_out: 48\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic convolution without padding\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "out_channels = 33\n",
    "kernel_size = 3\n",
    "L_in = 50  # Input sequence length\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "x = torch.randn(batch_size, in_channels, L_in)\n",
    "output = conv1d(x)\n",
    "\n",
    "# Calculate expected output length: L_out = (L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1\n",
    "L_out_expected = (L_in + 2*0 - 1*(kernel_size-1) - 1) // 1 + 1\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected L_out: {L_out_expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf0b2f29-7ca5-4397-86db-edfce62a1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n",
      "Length preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Convolution with padding to maintain spatial dimension\n",
    "# For kernel_size=3, padding=1 maintains the length (with stride=1)\n",
    "conv1d_same = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Length preserved: {x.shape[2] == output.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cdef7b-db36-4c24-86ec-062b584ba662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using padding='same' (requires stride=1)\n",
    "conv1d_same_str = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=5, padding='same')\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same_str(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b44ee49-aaa2-4797-9aa8-0f15c580aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 24])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Convolution with stride > 1 (downsampling)\n",
    "conv1d_stride = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, stride=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_stride(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 1*(3-1) - 1) // 2 + 1 = (50 - 2 - 1) // 2 + 1 = 47//2 + 1 = 23 + 1 = 24\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8463110e-6e6a-4ded-84f3-9160e63f4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 46])\n",
      "Effective kernel size with dilation=2: 5\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Dilated convolution (for larger receptive field)\n",
    "conv1d_dilated = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, dilation=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_dilated(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 2*(3-1) - 1) / 1 + 1 = (50 - 4 - 1) + 1 = 46\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Effective kernel size with dilation=2: {2*(3-1)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf59a20-95b9-434e-b493-0bb69e661f1e",
   "metadata": {},
   "source": [
    "### nn.Conv2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
