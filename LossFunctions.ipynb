{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117db1250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a83f4-1a91-4c68-b1b7-90bd0e4ef5ba",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">If we build a model that predicts an output quantity based on some input, we need a way to  **measure how the predicted quantity differs from the true one (target)**. Let $y_{\\rm true}$ be the true value for a given input, and $y_{\\rm pred}$ the corresponding prediction from our model. The **loss function** $L$ is a function that quantifies the differens between $y_{\\rm true}$ and $y_{\\rm pred}$. For defining a loss function the **type and shape of the output** play a crucial role.\n",
    "\n",
    "Let's now investigate various types of the loss function appearing in PyTorch package. In what follows:\n",
    "\n",
    "1. The index $b$ always denotes the sample number in a considered batch of size $B$.\n",
    "2. $N$ is the number of elements across the *entire predicted tensor* (batch + all other dimensions). It can be obtained through, e.g., $N = \\texttt{y\\_pred.numel()}$.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dccf9c-cdfe-4cc2-9bbe-31b83b5e1fb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Continuous-valued outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fe59e-4da8-41b7-a811-a25ebe47c3a7",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "This table summarizes common regression loss functions in PyTorch, their key properties, and typical use cases.\n",
    "\n",
    "| Loss Function | Basic Properties | Best Used For | Not Recommended For |\n",
    "|---------------|------------------|---------------|---------------------|\n",
    "| **L1Loss** | Computes mean absolute error; robust to outliers; produces sparse gradients | Datasets with outliers; when you want less sensitivity to extreme values | When outliers contain important information; smooth optimization paths |\n",
    "| **MSELoss** | Computes mean squared error; penalizes large errors heavily; smooth gradients | General regression tasks; normally distributed errors; when large errors should be heavily penalized | Data with many outliers (very sensitive); when all errors should be treated equally |\n",
    "| **HuberLoss** | Combines L1 and L2 loss; quadratic for small errors, linear for large ones; controlled by delta parameter | Datasets with moderate outliers; balance between MSE smoothness and L1 robustness | When you need pure L1 or L2 behavior; computational efficiency is critical |\n",
    "| **SmoothL1Loss** | Similar to Huber but with different transition; smooth everywhere; less sensitive to outliers than MSE | Object detection and localization tasks; bounding box regression | Pure regression without geometric constraints; when outliers are extremely rare |\n",
    "| **GaussianNLLLoss** | Models heteroscedastic uncertainty; predicts both mean and variance; negative log-likelihood for Gaussian distribution | Uncertainty estimation; when prediction confidence matters; heteroscedastic noise | Simple point predictions; when variance is constant; classification tasks |\n",
    "| **PoissonNLLLoss** | Based on Poisson distribution; for count data; handles non-negative integer outputs | Count prediction (events, items); rare event modeling; rate estimation | Continuous unbounded data; negative values; binary outcomes |\n",
    "| **KLDivLoss** | Measures divergence between probability distributions; asymmetric; expects log-probabilities as input | Distillation; distribution matching; variational inference; comparing probability distributions | Direct regression; when inputs aren't probability distributions; symmetric distance needed |\n",
    "\n",
    "Detailed explanations of each loss function, including their mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8232a-9c06-432e-8fdd-49eeb0c06dee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **L1Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914cb25-0989-4558-934b-896b4b2240fc",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The L1Loss is primarily used in regression tasks and most useful when the dataset contains outliers, as it is more robust to extreme values than L2 (MSE) loss, providing stable gradients for noisy data. It is applied, as we will see, in tasks like image reconstruction or when we want errors to scale linearly, not quadratically, which can be of big importance when the errors are large in magnitude.\n",
    "\n",
    "- Consider an output of arbitrary shape $y_{\\rm pred}$ and a target of the same shape $y_{\\rm true}$, then the L1Loss defines the following loss tensor:\n",
    "\n",
    "\n",
    "   $$\n",
    "  \\begin{aligned}\n",
    "{L_{\\rm L1}}^{b, i,j, \\cdots} = \\left| {y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right|\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "  \n",
    "- Moreover, the L1Loss can define the following two scalar loss functions:\n",
    "   1. Scalar loss function for the mean absolute error (MAE) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} \\, {L_{\\rm L1}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "   2. Scalar loss function for the absolute error (**without taking the mean**) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm L1}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c357bd65-c328-4755-a914-1e5420c979e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# The corresponding the loss tensor is obtained through:\n",
    "L1_LossTensor = torch.abs(y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c617148-3e7c-4530-bc86-a5b6aa8d6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.134202480316162\n",
      "1.134202480316162\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.L1Loss contains the default argument: reduction='mean'\n",
    "# This choice generates for us the MAE scalar loss function as explained above. The corresponding loss is:\n",
    "loss = nn.L1Loss()\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would apply aligned (1) directly: \n",
    "print(L1_LossTensor.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea504912-94c9-42e5-bca3-6b0824c33e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1190.91259765625\n",
      "1190.91259765625\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.L1Loss(reduction='sum')\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would just sum all elements of L1_LossTensor:\n",
    "print(L1_LossTensor.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a3817f-534b-4e22-9c6c-7a6c6d5fc651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.L1Loss(reduction='none')\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with L1_LossTensor, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all(L1_LossTensor - loss_tensor == torch.zeros(y_true.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02f7fd-6999-49f2-b4bd-a9e9b69152c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **MSELoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3caaf0-3e3c-4f04-a519-7f1c0ecebc18",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The MSE (also called L2) is primarily used in regression tasks where large deviations should be penalized more heavily, because the squared error grows faster for bigger differences between predicted and true values. This property makes it suitable when we want the model to prioritize reducing large mistakes, and it provides smooth gradients that facilitate optimization.\n",
    "\n",
    "- Consider an output of arbitrary shape $y_{\\rm pred}$ and a target of the same shape $y_{\\rm true}$, then the MSELoss defines the following loss tensor:\n",
    "  \n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{L_{\\rm MSE}}^{b, i,j, \\cdots} = \\left( {y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right)^2\n",
    "\\end{aligned}\n",
    "   $$\n",
    "- Moreover, the MSELoss can define the following two scalar loss functions:\n",
    "   1. Scalar loss function for the mean squared error between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm MSE}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "   2. Scalar loss function for the squared error (**without taking the mean**) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm MSE}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4374d5-8c37-43f9-b9e2-104496445f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# The corresponding the loss tensor is obtained through:\n",
    "L2_LossTensor = (y_pred - y_true)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a61ec1-c184-4704-89ca-008bfffebfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8992314338684082\n",
      "1.8992314338684082\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.MSELoss contains the following default argument: reduction='mean'\n",
    "# This choice generates for us the mean squared error. The corresponding loss is:\n",
    "loss = nn.MSELoss()\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would take the mean of L2_LossTensor\n",
    "print( L2_LossTensor.mean().item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958db209-1baf-46e7-812f-446562529e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994.1929931640625\n",
      "1994.1929931640625\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would some all elements of L2_LossTensor\n",
    "print( L2_LossTensor.sum().item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a547aa5-2703-44d7-900d-56cb3307a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with L2_LossTensor, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all(L2_LossTensor - loss_tensor == torch.zeros(y_true.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f120f1-920a-4dad-86d9-117fa693a4cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **HuberLoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbe3a0-425d-464f-b27f-339f5919a5d0",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The Huber loss is commonly used in regression tasks where robustness to outliers is important. It behaves like the mean squared error for small prediction errors, providing smooth gradients and stable optimization, while transitioning to a linear penalty for large errors, which reduces the influence of outliers. This combination makes the Huber loss well suited for problems where most outputs are well-behaved but occasional large deviations or noisy labels are present.\n",
    "\n",
    "- Consider an output of arbitrary shape $y_{\\rm pred}$ and a target of the same shape $y_{\\rm true}$, then the HuberLoss takes an additional number $\\delta$ defines the following loss tensor:\n",
    "  \n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{L_h}^{b, i,j, \\cdots}_\\delta &=\n",
    "\\frac{1}{2}\\times \\left({y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right)^2 \n",
    "\\times\n",
    "\\Theta\\left( \\delta - \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| \\right)\n",
    "\\\\\n",
    "&+\n",
    "\\delta\\times \\left( \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| - \\frac{1}{2} \\delta \\right)\n",
    "\\times\n",
    "\\Theta\\left( \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| - \\delta  \\right)\n",
    "\\end{aligned}\n",
    "   $$\n",
    "  where $\\Theta$ is the Heaviside function, defined as follows:\n",
    " \n",
    "   $$\n",
    "\\begin{aligned}\n",
    "\\Theta(x) =\n",
    "\\begin{cases}\n",
    "1, & x > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "- Moreover, the HuberLoss can define the following two scalar loss functions:\n",
    "   1. Avaraged scalar loss function:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_h}^{b, i,j, \\cdots}_\\delta\n",
    "\\end{aligned}\n",
    "   $$\n",
    "   2. Scalar loss function withouth avaraging:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_h}^{b, i,j, \\cdots}_\\delta\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d672e174-f931-4919-980a-9cbb4b1ac45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# We give delta now some value\n",
    "delta = 2\n",
    "# The corresponding the loss tensor is obtained through:\n",
    "error = y_pred - y_true\n",
    "Huber_LossTensor = torch.where(error.abs() < delta, 0.5 * (error**2), (delta * (error.abs() - 0.5 * delta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb2adc5-dca7-4646-b6c4-3cdc14f85c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0335075855255127\n",
      "1.0335075855255127\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.HuberLoss contains the following default argument: reduction='mean'\n",
    "# This choice generates for us avaraged scalar loss function as explained above:\n",
    "loss = nn.HuberLoss(delta=delta)\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as would take the mean of Huber_LossTensor:\n",
    "print(Huber_LossTensor.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26fca77-c5d5-4e00-b4dd-cff1ba8a238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085.1829833984375\n",
      "1085.1829833984375\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.HuberLoss(reduction='sum', delta=delta)\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This is the same as summing all elements of Huber_LossTensor\n",
    "print(Huber_LossTensor.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf03f96a-d610-485a-82a5-b3be92ba61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.HuberLoss(reduction='none', delta=delta)\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with Huber_LossTensor, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all( Huber_LossTensor - loss_tensor == torch.zeros(y_true.shape)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a7d86-9da3-4452-958b-33912a54da19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90cf3ba9-e692-490c-84c2-517c80329ecf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **SmoothL1Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbff93-e7a5-4a19-a19f-ded90dddb9b1",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The SmoothL1 loss is commonly used in regression tasks where robustness to outliers is important. It behaves like the mean squared error for small prediction errors, providing smooth gradients and stable optimization, while transitioning to a linear penalty for large errors, which reduces the influence of outliers. This combination makes the SmoothL1Loss loss well suited for problems where most outputs are well-behaved but occasional large deviations or noisy labels are present.\n",
    "\n",
    "- Consider an output of arbitrary shape $y_{\\rm true}$ and a target of the same shape $y_{\\rm true}$, then the SmoothL1 takes an additional number $\\beta$ defines the following loss tensor:\n",
    "  \n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{Ls}^{b, i,j, \\cdots}_\\beta &=\n",
    "\\frac{1}{2 \\beta}\\times \\left({y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right)^2 \n",
    "\\times\n",
    "\\Theta\\left( \\beta - \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| \\right)\n",
    "\\\\\n",
    "&+\n",
    "\\left( \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| - \\frac{1}{2} \\beta \\right)\n",
    "\\times\n",
    "\\Theta\\left( \\left|{y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right| - \\beta  \\right)\n",
    "\\end{aligned}\n",
    "   $$\n",
    "    where $\\Theta$ is the Heaviside function explained in the previous section.\n",
    "\n",
    "- Moreover, the SmoothL1 can define the following two scalar loss functions:\n",
    "   1. Avaraged scalar loss function:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {Ls}^{b, i,j, \\cdots}_\\beta\n",
    "\\end{aligned}\n",
    "   $$\n",
    "   2. Scalar loss function withouth avaraging:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {Ls}^{b, i,j, \\cdots}_\\beta\n",
    "\\end{aligned}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f110c45-bd64-4aa9-b386-4dd7bd1b3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# We give delta now some value\n",
    "beta = 3\n",
    "# The corresponding the loss tensor is obtained through:\n",
    "error = y_pred - y_true\n",
    "SmoothL1_LossTensor = torch.where(error.abs() < beta, 0.5 * (error**2)/beta, (error.abs() - 0.5 * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d97596-4b50-4b1f-9ad5-e477baa158c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3367563486099243\n",
      "0.3367563486099243\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.SmoothL1Loss contains the following default argument: reduction='mean'\n",
    "# This choice generates for us avaraged scalar loss function as explained above:\n",
    "loss = nn.SmoothL1Loss(beta = beta)\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as would take the mean of Huber_LossTensor:\n",
    "print(SmoothL1_LossTensor.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f0368a-2f76-48b8-86f7-90028df6abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353.59417724609375\n",
      "353.59417724609375\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.SmoothL1Loss(reduction='sum', beta=beta)\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This is the same as summing all elements of SmoothL1_LossTensor\n",
    "print(SmoothL1_LossTensor.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a232c0e-12f7-4e06-a0b0-11308d26a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.SmoothL1Loss(reduction='none', beta=beta)\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with SmoothL1_LossTensor, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all( SmoothL1_LossTensor - loss_tensor == torch.zeros(y_true.shape)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a6a90-fd0f-4558-a604-30a097814886",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **GaussianNLLLoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e4b9a-51f0-479c-b372-e1c6a99c11fa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "- If our target is expected to be sampled from Gaussian distributions and we build a neural network that predicts for us the **mean** and the **variance**, then we can use the GaussianNLLLoss to compute the loss on the mean and variacne on the same time. To use the GaussianNLL loss function we need the following components:\n",
    "     1. The target tensor ${{y}_{\\rm true}}$ of shape $(B, *)$, where $*$ we mean some arbitrary shape.\n",
    "     2. The mean tensor ${{y}_{\\rm pred}}$, predicted by the neural network, of shape $(B, *)$.\n",
    "     3. The variance tensor ${{y}_{\\rm var}}$, predicted by the neural network, of shape $(B, *)$.\n",
    "- It is worthy mentioning, that in Pytorch, both **target** and **variance** can use broadcasting (dimension=1 gets repeated to match the mean's shape).\n",
    "\n",
    "- The GaussianNLLLoss can give us the following tensor-valued error:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{L_g}^{b, i,j, \\cdots} &=   \\frac{1}{2}\\log\\left({\\rm max}\\left({y_{\\rm var}}^{b,i,j, \\cdots},\\, \\epsilon  \\right) \\right) + \\frac{\\left( {y_{\\rm true}}^{b,i,j, \\cdots} - {y_{\\rm pred}}^{b,i,j, \\cdots} \\right)^2}{2 {\\rm \\, max}\\left({y_{\\rm var}}^{b,i,j, \\cdots},\\, \\epsilon  \\right)}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "     where $\\epsilon$ is a small quantitiy needed for stability. Moreover, it also can give us the following two scalar loss functions\n",
    "\n",
    "   1. Avaraged scalar loss function:\n",
    "\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L &=  \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_g}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "   2. Scalar loss function without averaging:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L &=   \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_g}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "153ae0ff-7901-401c-a0e7-60144cd8bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities (i.e., the means) to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_var = torch.abs(torch.randn((10, 3, 5, 7)))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# The corresponding the loss tensor is obtained through:\n",
    "error = y_pred - y_true\n",
    "max_var = torch.clamp(y_var, min=1e-6)\n",
    "GaussianNLLL_Tensor = 0.5 * (torch.log(max_var) + error**2 / max_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e85b97-3ff7-4f55-bb7b-ce61d6a39a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.570807456970215\n",
      "8.570807456970215\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.GaussianNLLLoss contains the following default arguments: reduction='mean', epsilon 1e-06\n",
    "# To generate the mean scalar loss function we do the following:\n",
    "loss = nn.GaussianNLLLoss()\n",
    "print(loss(y_pred, y_true, y_var).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(GaussianNLLL_Tensor.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de75219c-5da0-42ba-80a2-0621c0acec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8999.34765625\n",
      "8999.34765625\n"
     ]
    }
   ],
   "source": [
    "# If we want the scalar loss function but without mean, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.GaussianNLLLoss(reduction='sum')\n",
    "print(loss(y_pred, y_true, y_var).item())\n",
    "# Or we can also obtain it manually as follows:\n",
    "print(GaussianNLLL_Tensor.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "275c58a2-2c00-4cff-be64-b0c2de949412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.GaussianNLLLoss(reduction='none')\n",
    "loss_tensor = loss(y_pred, y_true, y_var)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with GaussianNLLL_Tensor, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all( GaussianNLLL_Tensor - loss_tensor == torch.zeros(y_true.shape)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719e0d5-f5ef-45cf-b189-b474f8ef71c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab2a180-8625-4f1c-8593-58ecd4653db1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **PoissonNLLLoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b9051-28a8-4070-82ac-b7b0ad41d33f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "- If our target is expected to be sampled from Poisson distribution (non-negative count or continuous data) and we build a neural network that predicts the rate parameter $\\lambda$ (the average number of events expected to occur in a fixed interval of time or space), then we can use the PoissonNLLLoss to compute the loss in obtaining this rate parameter. To use the GaussianNLL loss function we need the following components:\n",
    "     1. The target tensor $y_{\\rm true}$ of arbitrary shape.\n",
    "     2. The log-rate tensor $\\log(\\lambda)$ predicted by the neural network, and it should have the same shape and the target tensor, i.e., each element in the target has its own log-rate.\n",
    "\n",
    "- The PoissonNLLLoss can give us the following two tensor-valued error:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{L_{p_1}}^{b, i,j, \\cdots} &= \\exp\\left(  {\\lambda^{b,}}^{b,i,j, \\cdots} \\right) - {\\lambda^{b,}}^{b,i,j, \\cdots} \\times {{y^{b,}}_{\\rm true}}^{b,i,j, \\cdots}\n",
    "\\\\\n",
    "{L_{p_2}}^{b, i,j, \\cdots} &= {\\lambda^{b,}}^{b,i,j, \\cdots} -  {{y^{b,}}_{\\rm true}}^{b,i,j, \\cdots} \\times \\log\\left({\\lambda^{b,}}^{b,i,j, \\cdots} + \\epsilon \\right)\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "     where $\\epsilon$ is a small quantitiy needed for stability. Moreover, it also can give us from each of the tensor losses above the following two scalar loss functions\n",
    "\n",
    "   1. Avaraged scalar loss functions:\n",
    "\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L_1 &=  \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{p_1}}^{b, i,j, \\cdots}\n",
    "\\\\\n",
    "L_2 &=  \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{p_2}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "  \n",
    "   2. Scalar loss functions without averaging:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L_1 &=   \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{p_1}}^{b, i,j, \\cdots}\n",
    "\\\\\n",
    "L_2 &=   \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{p_2}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89e1ea03-d329-42d0-a3c0-1efa0427608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities (i.e., the rate parametes) to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "lmbda = torch.abs(torch.randn((10, 3, 5, 7)))\n",
    "y_true = torch.randn((10, 3, 5, 7))\n",
    "# The corresponding the loss tensors is obtained through:\n",
    "epsilon = 1e-8\n",
    "Lp1 = torch.exp(lmbda) - lmbda * y_true\n",
    "Lp2 = lmbda - y_true * torch.log(lmbda + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b144ff5-e3bc-4d1f-b4b7-d62a37f7b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.695701837539673\n",
      "2.695701837539673\n",
      "0.8175972104072571\n",
      "0.8175972104072571\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.PoissonNLLLoss contains the following default arguments: log_input=True, reduction='mean', epsilon 1e-08\n",
    "# To generate the mean scalar loss function L1 we do the following:\n",
    "loss = nn.PoissonNLLLoss()\n",
    "print(loss(lmbda, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lp1.mean().item())\n",
    "\n",
    "# To generate the mean scalar loss function L2 we set the log_input argument to False:\n",
    "loss = nn.PoissonNLLLoss(log_input=False)\n",
    "print(loss(lmbda, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lp2.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38019a54-a479-4218-92d9-6b8f0f1960ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2830.48681640625\n",
      "2830.48681640625\n",
      "858.47705078125\n",
      "858.47705078125\n"
     ]
    }
   ],
   "source": [
    "# To obtain the scalar loss functions without taking the mean, we set the default argument reduction from 'mean' to 'sum', i.e.:\n",
    "loss = nn.PoissonNLLLoss(reduction='sum')\n",
    "print(loss(lmbda, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lp1.sum().item())\n",
    "\n",
    "# To generate L2 we set the log_input argument to False:\n",
    "loss = nn.PoissonNLLLoss(log_input=False, reduction='sum')\n",
    "print(loss(lmbda, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lp2.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9c81acc-2943-4e1c-b58a-dc3217749621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n",
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.PoissonNLLLoss(reduction='none')\n",
    "loss_tensor = loss(lmbda, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with Lp1, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all(Lp1 - loss_tensor == torch.zeros(y_true.shape)) )\n",
    "\n",
    "\n",
    "loss = nn.PoissonNLLLoss(reduction='none', log_input=False)\n",
    "loss_tensor = loss(lmbda, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# To see that the resulting loss tensor is identical with Lp2, we subtract both tensors form each others and check if we get a zero tensor:\n",
    "print(torch.all(Lp2 - loss_tensor == torch.zeros(y_true.shape)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da468628-68c7-4364-aecc-740c00fcbddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4567ea98-f5d0-49ba-8e95-ae6698729e61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **KLDivLoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be27223-dec2-4c40-a49f-32cf860db392",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "KLDivLoss\n",
    "\n",
    "- If our target is a probability distribution and we build a neural network that predicts another probability distribution, then we can use the KLDivLoss to measure how much the predicted distribution diverges from the target distribution. To use the KLDivLoss function we need the following components:\n",
    "     1. The target probability tensor ${{y}_{\\rm true}}$ of arbitrary shape.\n",
    "     2. The predicted probability tensor ${{y}_{\\rm pred}}$, and it should have the same shape and the target tensor.\n",
    "\n",
    "- The KLDivLoss can give us the following two tensor-valued error:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "{L_{k_1}}^{b, i,j, \\cdots} = &= {y_{\\rm true}}^{b,i,j, \\cdots} \\times  \\left(\\log\\left( {y_{\\rm true}}^{b,i,j, \\cdots} \\right) -  \\log\\left({y_{\\rm pred}}^{b,i,j, \\cdots} \\right) \\right)\n",
    "\\\\\n",
    "{L_{k_2}}^{b, i,j, \\cdots} = &= \\exp\\left({y_{\\rm true}}^{b,i,j, \\cdots}\\right) \\times  \\left({y_{\\rm true}}^{b,i,j, \\cdots} -  \\log\\left({y_{\\rm pred}}^{b,i,j, \\cdots} \\right) \\right)\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "     Moreover, it also can give us the following two scalar loss functions coming from each of the tensor errors\n",
    "\n",
    "   1. Avaraged scalar loss functions:\n",
    "\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L_1 &=  \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{k_1}}^{b, i,j, \\cdots}\n",
    "\\\\\n",
    "L_2 &=  \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{k_2}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "  \n",
    "   2. Scalar loss functions without averaging:\n",
    "\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L_1 &=   \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{k_1}}^{b, i,j, \\cdots}\n",
    "\\\\\n",
    "L_2 &=   \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{k_2}}^{b, i,j, \\cdots}\n",
    "\\end{aligned}\n",
    "   $$\n",
    "**Important Note:** Both ${L_{k_1}}$ and ${L_{k_2}}$ compute the **same KL divergence mathematically**. The difference is only in the **input format**:\n",
    "   - ${L_{k_1}}$ (when `log_target=False`, default): Assumes $y_{\\rm true}$ is in **regular probability space** and $y_{\\rm pred}$ is in **log-probability space**\n",
    "   - ${L_{k_2}}$ (when `log_target=True`): Assumes both $y_{\\rm true}$ and $y_{\\rm pred}$ are in **log-probability space**\n",
    "\n",
    "Using `log_target=True` provides better numerical stability when your target is already in log-space, as it avoids the need to convert back to probability space internally.\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa56f48-0c2e-40d4-99c9-5c01b1500447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred_raw = torch.abs(torch.abs(torch.randn((10, 3, 5, 7)))) + 0.1\n",
    "y_true_raw = torch.abs(torch.randn((10, 3, 5, 7))) + 0.1\n",
    "\n",
    "# For L1\n",
    "# Target: regular probabilities\n",
    "# Input: LOG probabilities\n",
    "y_true = F.softmax(y_true_raw, dim=-1)\n",
    "y_pred_log = F.log_softmax(y_pred_raw, dim=-1)  # ← LOG space\n",
    "\n",
    "# Manual calculation\n",
    "\n",
    "Lk1 = y_true * (torch.log(y_true) - y_pred_log)\n",
    "\n",
    "\n",
    "# For L2\n",
    "# Target: LOG probabilities\n",
    "# Input: LOG probabilities\n",
    "y_true_log = F.log_softmax(y_true_raw, dim=-1)  # ← LOG space\n",
    "y_pred_log = F.log_softmax(y_pred_raw, dim=-1)  # ← LOG space\n",
    "\n",
    "# Manual calculation\n",
    "Lk2 = torch.exp(y_true_log) * (y_true_log - y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba459ee-1b8a-4a4b-85ae-48a851f2bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05183225870132446\n",
      "0.05183225870132446\n",
      "0.05183225870132446\n",
      "0.05183225870132446\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.KLDivLoss contains the following default arguments: log_target=False, reduction='mean'\n",
    "# To generate the mean scalar loss function L1 we do the following:\n",
    "loss = nn.KLDivLoss()\n",
    "print(loss(y_pred_log, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lk1.mean().item())\n",
    "\n",
    "# To generate the mean scalar loss function L2 we set the log_input argument to False:\n",
    "loss = nn.KLDivLoss(log_target=True)\n",
    "print(loss(y_pred_log, y_true_log).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lk2.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daad2844-0936-4827-9207-fb654aae0b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.42387008666992\n",
      "54.42387008666992\n",
      "54.42387008666992\n",
      "54.42387008666992\n"
     ]
    }
   ],
   "source": [
    "# To obtain the scalar loss functions without taking the mean, we set the default argument reduction from 'mean' to 'sum', i.e.:\n",
    "loss = nn.KLDivLoss(reduction='sum')\n",
    "print(loss(y_pred_log, y_true).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lk1.sum().item())\n",
    "\n",
    "# To generate the mean scalar loss function L2 we set the log_input argument to False:\n",
    "loss = nn.KLDivLoss(log_target=True, reduction='sum')\n",
    "print(loss(y_pred_log, y_true_log).item())\n",
    "# Manually, the above result can be obtained as follows\n",
    "print(Lk2.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400dad6d-bf1f-44eb-9226-7303cdbcb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "True\n",
      "torch.Size([10, 3, 5, 7])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.KLDivLoss(reduction='none')\n",
    "loss_tensor = loss(y_pred_log, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# Lets see if this gives us the same as Lk1 by using allclose\n",
    "print(torch.allclose(Lk1, loss_tensor) )\n",
    "\n",
    "\n",
    "loss = nn.KLDivLoss(reduction='none', log_target=True)\n",
    "loss_tensor = loss(y_pred_log, y_true_log)\n",
    "print(loss_tensor.shape)\n",
    "# Agaim, we use allclose to see if this gives the same as Lk2\n",
    "print(torch.allclose(Lk2, loss_tensor) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ac86c-e614-4985-a9ee-bb8670cbdc93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Discrete Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063fc99-4be8-4c21-ae90-11212a55dab3",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "This table summarizes common classification and ranking loss functions in PyTorch, their key properties, and typical use cases.\n",
    "\n",
    "| Loss Function | Basic Properties | Best Used For | Not Recommended For |\n",
    "|---------------|------------------|---------------|---------------------|\n",
    "| **CrossEntropyLoss** | Combines softmax and NLLLoss; expects raw logits; numerically stable; handles multi-class classification | Multi-class classification (mutually exclusive classes); standard classification tasks | Multi-label problems; binary classification (use BCEWithLogitsLoss); when probabilities are already computed |\n",
    "| **NLLLoss** | Negative log-likelihood; expects log-probabilities as input; no built-in softmax | When you need custom preprocessing before loss; when log-probabilities are already computed | Raw logits (use CrossEntropyLoss instead); when you need softmax built-in |\n",
    "| **BCELoss** | Binary cross-entropy; expects probabilities in [0,1]; requires manual sigmoid | Binary classification when probabilities are pre-computed; multi-label with sigmoid already applied | Raw logits (numerical instability); prefer BCEWithLogitsLoss for better stability |\n",
    "| **BCEWithLogitsLoss** | Combines sigmoid and BCELoss; expects raw logits; numerically stable | Binary classification; multi-label classification; imbalanced datasets (supports pos_weight) | Multi-class single-label problems; when probabilities are already computed |\n",
    "| **MultiLabelSoftMarginLoss** | Multi-label version of logistic loss; expects raw logits; treats each label independently | Multi-label classification; when labels are not mutually exclusive; text categorization | Single-label classification; when class dependencies matter; small number of classes |\n",
    "| **SoftMarginLoss** | Binary logistic loss with margin; expects raw predictions; labels are {-1, +1} | Binary classification with confidence margins; when using {-1,+1} labels; SVM-style objectives | Standard {0,1} binary classification; multi-class problems; when margin concept doesn't apply |\n",
    "| **MultiMarginLoss** | Multi-class hinge loss; margin-based; penalizes incorrect predictions; expects raw scores | Multi-class classification with margin enforcement; SVM-style multi-class; when you want geometric margin | Standard softmax-based classification; probability estimation; when cross-entropy works well |\n",
    "| **MarginRankingLoss** | Learns relative ranking; expects two inputs and ranking preference; margin-based | Ranking tasks; learning to rank; pairwise preference learning; recommendation systems | Standard classification; absolute value prediction; when ranking relationships don't exist |\n",
    "| **HingeEmbeddingLoss** | Metric learning loss; binary similarity; expects distances and labels {-1, +1} | Similarity learning; determining if pairs are similar/dissimilar; Siamese networks | Multi-class classification; when absolute distances matter more than similarity; complex relationships |\n",
    "| **CosineEmbeddingLoss** | Measures cosine similarity; expects embeddings and labels {-1, +1}; angle-based metric | Face recognition; semantic similarity; when angle matters more than magnitude; text similarity | Distance-based similarity; when magnitude is important; binary classification without embeddings |\n",
    "| **TripletMarginLoss** | Anchor-positive-negative triplets; learns relative distances; enforces margin between similar/dissimilar | Face recognition; person re-identification; metric learning; when you have triplet data | Standard classification; binary similarity; when triplet mining is difficult; small datasets |\n",
    "| **TripletMarginWithDistanceLoss** | Generalized triplet loss; custom distance functions; more flexible than TripletMarginLoss | Same as TripletMarginLoss but with non-Euclidean distances; custom similarity metrics | Standard classification; when Euclidean distance suffices (use TripletMarginLoss) |\n",
    "\n",
    "Detailed explanations of each loss function, including their mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145e8a4-98c2-4424-99c7-ed073986447f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8c84e-b248-4412-8d34-0734b8b54899",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "The CrossEntropyLoss is the most commonly used loss function for multi-class classification problems where each sample belongs to exactly one class. It combines a softmax activation and negative log-likelihood loss in a single, numerically stable operation. This loss function expects raw, unnormalized scores (logits) from the model, not probabilities. For $C$ classes, the output tensor $y_{\\rm pred}$ coming from the neural etwork can have in general the following shapes: $(C), (B, C)$ or $(B, C, d_1, d_2, \\cdots, d_k)$. While for the target tensor $y_{\\rm true}$ we can have two casses:\n",
    "\n",
    "1. $y_{\\rm true}$ contains class probabilities. In this case the shape of $y_{\\rm true}$ is the same as $y_{\\rm pred}$. Moreover, each element in the target tensor must be between $[0, 1]$ and the target data type is **required** to be float when using the class probabilities. In this case, the CrossEntropyLoss defines the following loss tensor:\n",
    "      $$\n",
    "\\begin{aligned} \n",
    "{L_{\\rm {CE_1}}}^{b,i,j,\\cdots} = -\\sum_{c=1}^{C} \\omega_c \\, y_{\\rm true}^{b,c, i,j,\\cdots} \\log \\left( \\frac{e^{y_{\\rm pred}^{b,c, i,j,\\cdots}}}{\\sum_{c'=1}^C e^{y_{\\rm pred}^{b,c', i,j,\\cdots}} } \\right) \n",
    "\\end{aligned}\n",
    "      $$\n",
    "   From the above loss tensor we can create the following two scalar losses:\n",
    "         $$\n",
    "\\begin{aligned} \n",
    "L_1 = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm {CE_1}}}^{b,i,j,\\cdots}\\,, \\quad L_2 = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm {CE_1}}}^{b,i,j,\\cdots}\n",
    "\\end{aligned}\n",
    "      $$\n",
    "2. $y_{\\rm true}$ contains class indices, i.e., its values are simply in $[0, C)$. In this case the shape can be: $(), (B)$ or $(B, d_1, d_2, \\cdots, d_k)$. Moreover, the target data type is **required** to be long when using class indices. In this case, the CrossEntropyLoss defines the following loss tensor:\n",
    "      $$\n",
    "\\begin{aligned} \n",
    "{L_{\\rm {CE_2}}}^{b,i,j,\\cdots} = - \\omega_{y_b} \\log \\left( \\frac{e^{y_{\\rm pred}^{b,y_b, i,j,\\cdots}}}{\\sum_{c'=1}^C e^{y_{\\rm pred}^{b,c', i,j,\\cdots}} } \\right)\\,, \\quad y_b := y_{\\rm true}^{b,i,j,\\cdots}\n",
    "\\end{aligned}\n",
    "      $$\n",
    "   From the above loss tensor we can create the following two scalar losses:\n",
    "         $$\n",
    "\\begin{aligned} \n",
    "L_1 = \\frac{1}{N \\Omega}\\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm {CE_2}}}^{b,i,j,\\cdots}\\,, \\quad L_2 = \\sum_{b=0}^{B-1} \\sum_{b,i,j, \\cdots} {L_{\\rm {CE_2}}}^{b,i,j,\\cdots}\n",
    "\\end{aligned}\n",
    "      $$\n",
    "    Where $\\omega_i$ are weights of shape $(C,)$ (below explained more), and $\\Omega$ is the sum over all weights.\n",
    "\n",
    "\n",
    "**Comments:**\n",
    "\n",
    "- **Class Weights (`weight` parameter)**: The optional `weight` parameter is a tensor of shape $(C,)$ that assigns a manual rescaling weight to each class. This is particularly useful for imbalanced datasets where some classes appear much more frequently than others. If provided, the weight $w_c$ for class $c$ multiplies the loss contribution of that class. For example, if class 0 appears 1000 times and class 1 appears 100 times in your dataset, you might set `weight=torch.tensor([0.1, 1.0])` to give more importance to the rare class. Note that weights are applied **before** any reduction operation.\n",
    "\n",
    "- **Ignore Index (`ignore_index` parameter)**: There is an `ignore_index` parameter (default: -100) specifies a target class index that should be ignored when computing the loss. This is useful in scenarios like:\n",
    "  - Padding tokens in NLP tasks where certain positions don't represent real data\n",
    "  - Unlabeled pixels in semantic segmentation\n",
    "  - Masked or missing labels in your dataset\n",
    "  \n",
    "  When a target has a value equal to `ignore_index`, that position contributes 0 to the loss and is excluded from the count when computing the mean (i.e., it doesn't appear in the denominator). In the formula for case 2 with `reduction='mean'`, the denominator becomes:\n",
    "  $$\n",
    "  \\sum_{n=1}^{N} w_{y_n} \\cdot \\mathbb{1}_{\\{y_n \\neq \\text{ignore\\_index}\\}}\n",
    "  $$\n",
    "  where $\\mathbb{1}_{\\{\\cdot\\}}$ is an indicator function that equals 1 when the condition is true and 0 otherwise. This ensures that ignored positions don't affect the average loss value.\n",
    "\n",
    "- **Label Smoothing (`label_smoothing` parameter)**: When `label_smoothing > 0`, the target probabilities are smoothed by distributing a small amount of probability mass away from the peak values to all other classes. This technique is **only applicable when targets are provided as class probabilities (case 1)**, not when targets are class indices (case 2). Specifically, for each position $(b, c, i, j, \\cdots)$ in the target probability tensor $y_{\\rm true}^{b,c,i,j,\\cdots}$, the smoothed target becomes:\n",
    "  $$\n",
    "  {y_{\\rm true, smooth}}^{b,c,i,j,\\cdots} = y_{\\rm true}^{b,c,i,j,\\cdots} \\cdot (1 - \\epsilon) + \\frac{\\epsilon}{C}\n",
    "  $$\n",
    "  where $\\epsilon$ is the `label_smoothing` value. This formula takes the original probability for class $c$ and shrinks it toward a uniform distribution over all classes. The smoothed probabilities ${y_{\\rm true, smooth}}^{b,c,i,j,\\cdots}$ are then used in place of $y_{\\rm true}^{b,c,i,j,\\cdots}$ in the case 1 loss formula. This technique can help prevent overconfidence and improve model generalization by encouraging the model to maintain some uncertainty even for the most likely classes.\n",
    "\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527bfa9-a3a7-4097-8a21-62789feec47e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Without spatial dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10691c54-c12c-491d-8da4-dd3f01c3a3d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Targets as probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef224c09-6495-4f74-b8e4-75e68dd4e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Let us generate some logits and true labels randomly. We take 5 classes and a batch size of 10:\n",
    "C = 5  # number of classes\n",
    "B = 10  # batch size\n",
    "weights = torch.randn(C, dtype=torch.float32)\n",
    "y_pred = torch.randn((B, C), dtype=torch.float32)  # raw logits, can be any real numbers\n",
    "\n",
    "# y_true: target probabilities (must be float, sum to 1 along class dimension)\n",
    "\n",
    "# Option 1: Create random probabilities that sum to 1\n",
    "y_true_raw = torch.rand((B, C), dtype=torch.float32) # random values in [0, 1]\n",
    "y_true = y_true_raw / y_true_raw.sum(dim=1, keepdim=True)  # normalize so each row sums to 1\n",
    "\n",
    "# Now create the LCE1 tensor:\n",
    "\n",
    "LCE1_Tensor = - weights * y_true * torch.log(F.softmax(y_pred, dim=1))\n",
    "LCE1_Tensor = LCE1_Tensor.sum(dim=1)\n",
    "\n",
    "# Now, let's verify that we can reproduce LCE1_Tensor with nn.CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights, reduction='none')\n",
    "pytorch_loss_tensor = loss_fn(y_pred, y_true)\n",
    "\n",
    "print(torch.allclose(pytorch_loss_tensor, LCE1_Tensor, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db1472d0-ae01-4829-adef-9c591c69340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Now let's reproduce L1 from LCE1:\n",
    "L1 = LCE1_Tensor.mean()\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "\n",
    "print((loss(y_pred, y_true)-L1).item())\n",
    "\n",
    "\n",
    "\n",
    "# Now let's reproduce L2 from LCE1:\n",
    "L2 = LCE1_Tensor.sum()\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=weights, reduction='sum')\n",
    "\n",
    "print((loss(y_pred, y_true)-L2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be20af3-9da3-43dc-b0d9-8bbc92dfe919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b118de86-87b3-4a10-9f39-fb042bbf3750",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Targets as indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b1e04b6-3e39-45e0-b323-96c58f9e0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Let us generate some logits and true labels randomly. We take 5 classes and a batch size of 10:\n",
    "C = 5  # number of classes\n",
    "B = 10  # batch size\n",
    "weights = torch.randn(C, dtype=torch.float32)\n",
    "y_pred = torch.randn((B, C), dtype=torch.float32)  # raw logits, can be any real numbers\n",
    "\n",
    "# y_true: target probabilities (must be float, sum to 1 along class dimension)\n",
    "\n",
    "# Option 1: Create random probabilities that sum to 1\n",
    "y_true = torch.randint(0, C, (B,), dtype=torch.long)  # dtype MUST be long! It goes from 0 to C-1\n",
    "\n",
    "# Now create the LCE1 tensor:\n",
    "LCE2_Tensor = -weights[y_true] * torch.log(F.softmax(y_pred, dim=1))[range(B), y_true]\n",
    "\n",
    "# Now, let's verify that we can reproduce LCE2_Tensor with nn.CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights, reduction='none')\n",
    "pytorch_loss_tensor = loss_fn(y_pred, y_true)\n",
    "\n",
    "print(torch.allclose(pytorch_loss_tensor, LCE2_Tensor, atol=1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "30e13344-d9ca-40ff-92c9-3694c4868fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Now let's reproduce L1 from LCE2:\n",
    "L1 = LCE2_Tensor.sum() /(weights[y_true].sum())\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "\n",
    "loss_dif = (loss(y_pred, y_true)-L1).item()\n",
    "print(loss_dif < 1e-6)\n",
    "\n",
    "\n",
    "# Now let's reproduce L2 from LCE1:\n",
    "L2 = LCE2_Tensor.sum()\n",
    "loss = nn.CrossEntropyLoss(weight=weights, reduction='sum')\n",
    "loss_dif = (loss(y_pred, y_true)-L2).item()\n",
    "print(loss_dif < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad0d76-88bc-4d53-a3d5-f6accdf1e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
