{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1114011f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a83f4-1a91-4c68-b1b7-90bd0e4ef5ba",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">If we build a model that predicts an output quantity based on some input, we need a way to  **measure how the predicted quantity differs from the true one**. Let $y_{\\rm true}$ be the true value for a given input, and $y_{\\rm pred}$ the corresponding prediction from our model. The **loss function** $L$ is a function that quantifies the differens between $y_{\\rm true}$ and $y_{\\rm pred}$. For defining a loss function the **type and shape of the output** play a crucial role.\n",
    "\n",
    "Let's now investigate various types of the loss function appearing in PyTorch package:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dccf9c-cdfe-4cc2-9bbe-31b83b5e1fb1",
   "metadata": {},
   "source": [
    "# Continuous-valued Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8232a-9c06-432e-8fdd-49eeb0c06dee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **L1Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914cb25-0989-4558-934b-896b4b2240fc",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "- Consider an output of arbitrary shape, then the L1Loss can define for us the following three types of loss functions:\n",
    "\n",
    "   1. Scalar loss function for the mean absolute error (MAE) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{equation} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\left| {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| \\hspace{3cm} (1)\n",
    "\\end{equation}\n",
    "   $$\n",
    "\n",
    "        where $b$ denotes the sample number in the considered batch of size $B$, $N$ is the total number of elements in the output batch, i.e., $N = \\texttt{y\\_pred.numel()} = \\texttt{y\\_true.numel()}$.\n",
    "\n",
    "\n",
    "   2. Scalar loss function for the absolute error (**without taking the mean**) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{equation} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\left| {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| \\hspace{3.5cm} (2)\n",
    "\\end{equation}\n",
    "   $$\n",
    "   3. Tensor-valued loss function of the same shape as the output quantity:\n",
    "      $$\n",
    "\\begin{equation} \n",
    "{L^b}^{i,j, \\cdots} = \\left| {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| \\hspace{4.2cm} (3)\n",
    "\\end{equation}\n",
    "   $$\n",
    "\n",
    "- The L1Loss is primarily used in regression tasks and most useful when the dataset contains outliers, as it is more robust to extreme values than L2 (MSE) loss, providing stable gradients for noisy data. It is applied, as we will see, in tasks like image reconstruction or when we want errors to scale linearly, not quadratically, which can be of big importance when the errors are large in magnitude.\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c357bd65-c328-4755-a914-1e5420c979e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c617148-3e7c-4530-bc86-a5b6aa8d6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1144014596939087\n",
      "1.1144014596939087\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.L1Loss contains the following default argument: reduction='mean'\n",
    "# This choice generates for us the MAE scalar loss function as explained above. The corresponding loss is:\n",
    "loss = nn.L1Loss()\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would apply equation (1) directly: \n",
    "print(torch.abs(y_pred-y_true).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea504912-94c9-42e5-bca3-6b0824c33e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170.12158203125\n",
      "1170.12158203125\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.L1Loss(reduction='sum')\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would apply equation (2) directly: \n",
    "print(torch.abs(y_pred-y_true).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5a3817f-534b-4e22-9c6c-7a6c6d5fc651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.L1Loss(reduction='none')\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# This gives us the same as if we would apply equation (3) directly: \n",
    "print(torch.all(torch.abs(y_pred - y_true) - loss_tensor == torch.zeros(y_true.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02f7fd-6999-49f2-b4bd-a9e9b69152c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **MSELoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3caaf0-3e3c-4f04-a519-7f1c0ecebc18",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "- Consider an output of arbitrary shape, then the MSELoss can define for us the following three types of loss functions:\n",
    "\n",
    "   1. Scalar loss function for the mean squared error (MSE) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{equation} \n",
    "L = \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\left( {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \\hspace{3cm} (1)\n",
    "\\end{equation}\n",
    "   $$\n",
    "\n",
    "        where $b$ denotes the sample number in the considered batch of size $B$, $N$ is the total number of elements in the output batch, i.e., $N = \\texttt{y\\_pred.numel()} = \\texttt{y\\_true.numel()}$.\n",
    "\n",
    "\n",
    "   2. Scalar loss function for the squared error (**without taking the mean**) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{equation} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\left( {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \\hspace{3.5cm} (2)\n",
    "\\end{equation}\n",
    "   $$\n",
    "   3. Tensor-valued loss function of the same shape as the output quantity:\n",
    "      $$\n",
    "\\begin{equation} \n",
    "{L^b}^{i,j, \\cdots} = \\left( {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \\hspace{4.2cm} (3)\n",
    "\\end{equation}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "- The MSE is primarily used in regression tasks where large deviations should be penalized more heavily, because the squared error grows faster for bigger differences between predicted and true values. This property makes it suitable when we want the model to prioritize reducing large mistakes, and it provides smooth gradients that facilitate optimization.\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e4374d5-8c37-43f9-b9e2-104496445f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate some true and predicted quantities randomly. For that sake, we take the output quantities to be of shape (3, 5, 7)\n",
    "# and we take a batch size of 10:\n",
    "y_pred = torch.randn((10, 3, 5, 7))\n",
    "y_true = torch.randn((10, 3, 5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7a61ec1-c184-4704-89ca-008bfffebfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.864696979522705\n",
      "1.864696979522705\n"
     ]
    }
   ],
   "source": [
    "# The init method of nn.MSELoss contains the following default argument: reduction='mean'\n",
    "# This choice generates for us the MAE scalar loss function as explained above. The corresponding loss is:\n",
    "loss = nn.MSELoss()\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would apply equation (1) directly: \n",
    "print( ((y_pred-y_true)**2).mean().item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "958db209-1baf-46e7-812f-446562529e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957.931884765625\n",
      "1957.931884765625\n"
     ]
    }
   ],
   "source": [
    "# To obtain the non-averaged case, we have to change the default argument reduction from 'mean' to 'sum'), i.e.,\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "print(loss(y_pred, y_true).item())\n",
    "# This gives us the same as if we would apply equation (2) directly: \n",
    "print( ((y_pred-y_true)**2).sum().item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a547aa5-2703-44d7-900d-56cb3307a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5, 7])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the tensor-valued loss function, we have to change the default argument reduction from 'mean' to 'none':\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "loss_tensor = loss(y_pred, y_true)\n",
    "print(loss_tensor.shape)\n",
    "# This gives us the same as if we would apply equation (3) directly: \n",
    "print(torch.all( (y_pred-y_true)**2 - loss_tensor == torch.zeros(y_true.shape)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f120f1-920a-4dad-86d9-117fa693a4cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **HuberLoss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecff5f-7bed-4464-a190-6d03945bf657",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "- Consider an output of arbitrary shape, then the HuberLoss takes the predicted and true quantities together with an additional number $\\delta$ and defines for us the following three types of loss functions:\n",
    "\n",
    "   1. Scalar loss function for between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{aligned} \n",
    "L &= \\frac{1}{N} \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\Biggl\\{ \n",
    "\\frac{1}{2}\\times \\left({{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \n",
    "\\times\n",
    "\\Theta\\left( \\delta - \\left|{{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| \\right)\n",
    "\\\\\n",
    "&+\n",
    "\\delta\\times \\left( \\left|{{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| - \\frac{1}{2} \\delta \\right)\n",
    "\\times\n",
    "\\Theta\\left( \\left|{{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right| - \\delta  \\right)\n",
    "\\Biggr\\}\n",
    "\\hspace{3cm}(1)\n",
    "\\end{aligned}\n",
    "   $$\n",
    "\n",
    "        where $b$ denotes the sample number in the considered batch of size $B$, $N$ is the total number of elements in the output batch, i.e., $N = \\texttt{y\\_pred.numel()} = \\texttt{y\\_true.numel()}$. Moreover, $\\Theta$ is the Heaviside function, defined as follows:\n",
    " \n",
    "   $$\n",
    "\\begin{equation}\n",
    "\\Theta(x) =\n",
    "\\begin{cases}\n",
    "1, & x > 0 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\hspace{3cm}(2)\n",
    "\\end{equation}\n",
    "   $$      \n",
    "\n",
    "\n",
    "   3. Scalar loss function for the squared error (**without taking the mean**) between each element in the true and predicted tensors:\n",
    "   $$\n",
    "\\begin{equation} \n",
    "L = \\sum_{b=0}^{B-1} \\sum_{i,j, \\cdots} \\left( {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \\hspace{3.5cm} (2)\n",
    "\\end{equation}\n",
    "   $$\n",
    "   4. Tensor-valued loss function of the same shape as the output quantity:\n",
    "      $$\n",
    "\\begin{equation} \n",
    "{L^b}^{i,j, \\cdots} = \\left( {{y^b}_{\\rm true}}^{i,j, \\cdots} - {{y^b}_{\\rm pred}}^{i,j, \\cdots} \\right)^2 \\hspace{4.2cm} (3)\n",
    "\\end{equation}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "- The MSE is primarily used in regression tasks where large deviations should be penalized more heavily, because the squared error grows faster for bigger differences between predicted and true values. This property makes it suitable when we want the model to prioritize reducing large mistakes, and it provides smooth gradients that facilitate optimization.\n",
    "\n",
    "As next, let us investigate how this function can be used in PyTorch:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bfde6-4bfd-4c0e-b592-a4985ea9ef8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
